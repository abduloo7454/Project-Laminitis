{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# If you're in Colab and want SMOTE; otherwise you can skip this cell.\n",
        "!pip -q install imbalanced-learn"
      ],
      "metadata": {
        "id": "I4eUmSV9wzfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabpfn"
      ],
      "metadata": {
        "id": "SPJ3OP0W8-Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "GSrffuec-K0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YvLMsqPweeq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"iframe\"\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Laminitis_27_Oct/preprocessed.xlsx')\n",
        "# Loop only over object (string) columns\n",
        "for col in df.select_dtypes(include='number').columns:\n",
        "    num_missing = df[col].isna().sum()\n",
        "    if num_missing > 0:\n",
        "        print(f\"{col}: {num_missing} NaN values\")"
      ],
      "metadata": {
        "id": "mI6iPs6Owoge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your features and label are:\n",
        "x = df.drop(columns=['Class'])  # all features\n",
        "X = x.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "y = df['Class']"
      ],
      "metadata": {
        "id": "NW3X1KJewq5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = X.corr().abs()\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "# correlation_matrix = feat_corr.corr()\n",
        "\n",
        "# Create the heatmap with annotations (to show correlation values)\n",
        "plt.figure(figsize=(16, 14))  # Adjust the figure size for better clarity\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=0.8, cmap='coolwarm', annot_kws={\"size\": 10})\n",
        "\n",
        "# Add a title\n",
        "plt.title('Correlation Matrix', fontsize=18)\n",
        "\n",
        "# Rotate x-axis labels for better visibility\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Rotate y-axis labels for better visibility\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.savefig(\"Correlation matrix.pdf\",format='pdf', dpi=500)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WzHGIG_-7qKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upper triangle matrix of correlations\n",
        "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# List of correlated pairs\n",
        "high_corr = [\n",
        "    (column, idx, corr)\n",
        "    for column in upper.columns\n",
        "    for idx, corr in upper[column].items()\n",
        "    if corr > 0.7\n",
        "]\n",
        "pd.DataFrame(high_corr, columns=[\"Feature_1\", \"Feature_2\", \"Correlation\"])\n"
      ],
      "metadata": {
        "id": "pHNh08he7qHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.7\n",
        "\n",
        "# Drop one of each correlated pair\n",
        "to_drop = [\n",
        "    column for column in upper.columns if any(upper[column] > threshold)\n",
        "]\n",
        "X_uncorr = X.drop(columns=to_drop, errors='ignore')\n",
        "\n",
        "print(\"Dropped:\", to_drop)\n",
        "print(\"Remaining features:\", X_uncorr.shape[1])\n",
        "X = X_uncorr"
      ],
      "metadata": {
        "id": "0tcvKkgO7qD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_uncorr.columns"
      ],
      "metadata": {
        "id": "pdXaa56c1aY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "6toK1VV-7p3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "6pVMqlvj3WQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "mtFAqeGu3ePl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score, matthews_corrcoef\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "Zm7ytTK8z9Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        penalty='l2', solver='lbfgs', max_iter=1000, class_weight='balanced', random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=5, min_samples_split=5, class_weight='balanced', random_state=42\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"LogisticRegression\": logreg,\n",
        "    \"RandomForest\": rf,\n",
        "}"
      ],
      "metadata": {
        "id": "kW-kICgiz9Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import permutation_test_score\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# score, perm_scores, pvalue = permutation_test_score(\n",
        "#     rf, x, y, cv=cv, n_permutations=100, scoring=\"accuracy\", n_jobs=-1\n",
        "# )\n",
        "# print(f\"Permutation Accuracy: {score:.3f}, p-value: {pvalue:.5f}\")"
      ],
      "metadata": {
        "id": "luX729Co0MpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(x, y)\n",
        "importances = pd.Series(rf.feature_importances_, index=x.columns).sort_values(ascending=False)\n",
        "# print(importances.head(10))\n",
        "\n",
        "importances.plot(kind='bar', figsize=(8,4))\n",
        "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nBpaaruh0Mk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "FLkiuDuJXhVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from tabpfn import TabPFNClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "RNG = 42\n",
        "\n",
        "def is_binary(y):\n",
        "    return len(np.unique(y)) == 2\n",
        "\n",
        "def metric_avg(y_true):\n",
        "    # For multi-class: use macro avg so each class is weighted equally\n",
        "    return 'binary' if is_binary(y_true) else 'macro'\n",
        "\n",
        "def safe_proba(estimator, X):\n",
        "    # All models below support predict_proba when configured; this is just a guard\n",
        "    if hasattr(estimator, \"predict_proba\"):\n",
        "        return estimator.predict_proba(X)\n",
        "    # Fallback via decision function -> fake probs with min-max scaling (not ideal)\n",
        "    if hasattr(estimator, \"decision_function\"):\n",
        "        z = estimator.decision_function(X)\n",
        "        if z.ndim == 1:  # binary\n",
        "            from scipy.special import expit\n",
        "            p1 = expit(z)\n",
        "            return np.vstack([1 - p1, p1]).T\n",
        "        # multiclass: softmax\n",
        "        z = z - z.max(axis=1, keepdims=True)\n",
        "        ez = np.exp(z)\n",
        "        return ez / ez.sum(axis=1, keepdims=True)\n",
        "    raise ValueError(\"Estimator has neither predict_proba nor decision_function.\")"
      ],
      "metadata": {
        "id": "toWbUurCz9Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RNG, stratify=y\n",
        ")\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "n_classes = len(np.unique(y))\n",
        "classes = np.unique(y)\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape, \"Classes:\", classes)\n"
      ],
      "metadata": {
        "id": "OElMQMa1Xw_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"LogReg\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(max_iter=2000, class_weight='balanced', random_state=RNG))\n",
        "    ]),\n",
        "    \"SVM\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", SVC(kernel=\"rbf\", probability=True, class_weight='balanced', random_state=RNG))\n",
        "    ]),\n",
        "    \"KNN\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", KNeighborsClassifier(n_neighbors=5))\n",
        "    ]),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=400, max_depth=None, class_weight='balanced', random_state=RNG, n_jobs=-1\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=400, max_depth=5, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_lambda=1.0, objective=\"binary:logistic\" if n_classes==2 else \"multi:softprob\",\n",
        "        eval_metric=\"logloss\", random_state=RNG, n_jobs=-1\n",
        "    ),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(\n",
        "        max_depth=None, class_weight='balanced', random_state=RNG\n",
        "    ),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(\n",
        "        n_estimators=300, learning_rate=0.05, max_depth=3, random_state=RNG\n",
        "    ),\n",
        "    \"TabularPFN\":  TabPFNClassifier(),\n",
        "    \"ANN\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", MLPClassifier(hidden_layer_sizes=(64,32), activation=\"relu\",\n",
        "                              max_iter=500, random_state=RNG)),\n",
        "    ])\n",
        "}\n"
      ],
      "metadata": {
        "id": "OxV5zeEpX9tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# Remove TabularPFN from your models dictionary\n",
        "# ========================================\n",
        "models_to_remove = ['TabularPFN', 'TabularPFNClassifier']\n",
        "\n",
        "# Create a new models dictionary without TabularPFN\n",
        "filtered_models = {}\n",
        "for name, model in models.items():\n",
        "    if name not in models_to_remove:\n",
        "        filtered_models[name] = model\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Skipping {name} (requires authentication)\")\n",
        "\n",
        "# Use filtered_models instead of models\n",
        "models = filtered_models\n",
        "\n",
        "print(\"Remaining models:\", list(models.keys()))"
      ],
      "metadata": {
        "id": "PiOLFQb5Yilr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 3) 5-Fold Stratified Cross Validation + collect predictions\n",
        "# ========================================\n",
        "def is_binary(y): return len(np.unique(y)) == 2\n",
        "def avg_mode(y): return \"binary\" if is_binary(y) else \"macro\"\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def is_binary(y): return len(np.unique(y)) == 2\n",
        "def avg_mode(y): return \"binary\" if is_binary(y) else \"macro\"\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
        "\n",
        "final_results = []\n",
        "roc_data = {}         # {model: (fpr, tpr, auc)}\n",
        "conf_matrices = {}    # {model: cm}\n",
        "labels_global = np.unique(y)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    fold_metrics = []\n",
        "\n",
        "    # aggregate across folds\n",
        "    y_true_all, y_pred_all, y_proba_all = [], [], []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y), start=1):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # probabilities for ROC\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_proba = model.predict_proba(X_test)\n",
        "        elif hasattr(model, \"decision_function\"):\n",
        "            z = model.decision_function(X_test)\n",
        "            from scipy.special import expit\n",
        "            if z.ndim == 1:\n",
        "                y_proba = np.vstack([1 - expit(z), expit(z)]).T\n",
        "            else:\n",
        "                ez = np.exp(z - z.max(axis=1, keepdims=True))\n",
        "                y_proba = ez / ez.sum(axis=1, keepdims=True)\n",
        "        else:\n",
        "            # fallback: zeros; ROC will be meaningless; better ensure proba/decision_function exists\n",
        "            y_proba = np.zeros((len(y_pred), len(np.unique(y))))\n",
        "\n",
        "        # store for aggregate CM/ROC\n",
        "        y_true_all.extend(y_test)\n",
        "        y_pred_all.extend(y_pred)\n",
        "        # for ROC store positive-class prob in binary, else full matrix\n",
        "        if is_binary(y):\n",
        "            y_proba_all.extend(y_proba[:, 1])\n",
        "        else:\n",
        "            y_proba_all.extend(list(y_proba))  # keep rows\n",
        "\n",
        "        # per-fold metrics\n",
        "        avg = avg_mode(y)\n",
        "        acc  = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average=avg, zero_division=0)\n",
        "        rec  = recall_score(y_test, y_pred, average=avg, zero_division=0)\n",
        "        f1   = f1_score(y_test, y_pred, average=avg, zero_division=0)\n",
        "\n",
        "        if is_binary(y):\n",
        "            roc_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
        "        else:\n",
        "            from sklearn.preprocessing import label_binarize\n",
        "            classes_ = np.unique(y)\n",
        "            Y_test_bin = label_binarize(y_test, classes=classes_)\n",
        "            roc_auc = roc_auc_score(Y_test_bin, y_proba, average='macro', multi_class='ovr')\n",
        "\n",
        "        fold_metrics.append([acc, prec, rec, f1, roc_auc])\n",
        "        print(f\"Fold {fold}:  Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}, AUC={roc_auc:.3f}\")\n",
        "\n",
        "    # aggregate mean ¬± std\n",
        "    arr = np.array(fold_metrics)\n",
        "    mean = arr.mean(axis=0); std = arr.std(axis=0)\n",
        "    print(f\"Mean ¬± SD:  Acc={mean[0]:.3f}¬±{std[0]:.3f}, Prec={mean[1]:.3f}¬±{std[1]:.3f}, \"\n",
        "          f\"Rec={mean[2]:.3f}¬±{std[2]:.3f}, F1={mean[3]:.3f}¬±{std[3]:.3f}, AUC={mean[4]:.3f}¬±{std[4]:.3f}\")\n",
        "\n",
        "    final_results.append({\n",
        "        \"Model\": name,\n",
        "        \"Acc_mean\": mean[0], \"Acc_std\": std[0],\n",
        "        \"Prec_mean\": mean[1], \"Prec_std\": std[1],\n",
        "        \"Rec_mean\": mean[2], \"Rec_std\": std[2],\n",
        "        \"F1_mean\": mean[3], \"F1_std\": std[3],\n",
        "        \"AUC_mean\": mean[4], \"AUC_std\": std[4],\n",
        "    })\n",
        "\n",
        "    # ===== aggregate ROC and CM =====\n",
        "    from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "    y_true_all = np.array(y_true_all)\n",
        "    y_pred_all = np.array(y_pred_all)\n",
        "\n",
        "    if is_binary(y):\n",
        "        y_proba_all = np.array(y_proba_all)  # shape (N,)\n",
        "        fpr, tpr, _ = roc_curve(y_true_all, y_proba_all)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_data[name] = (fpr, tpr, roc_auc)\n",
        "    else:\n",
        "        # macro-avg ROC (OvR)\n",
        "        from sklearn.preprocessing import label_binarize\n",
        "        classes_ = np.unique(y)\n",
        "        Y_bin = label_binarize(y_true_all, classes=classes_)\n",
        "        Yp = np.vstack(y_proba_all)  # shape (N, C)\n",
        "        # compute macro curve by averaging TPR across classes on union FPR grid\n",
        "        from sklearn.metrics import roc_curve as _roc, roc_auc_score as _auc\n",
        "        fpr_all = []\n",
        "        tpr_all = []\n",
        "        for i in range(Y_bin.shape[1]):\n",
        "            fpr_i, tpr_i, _ = _roc(Y_bin[:, i], Yp[:, i])\n",
        "            fpr_all.append(fpr_i); tpr_all.append(tpr_i)\n",
        "        all_fpr = np.unique(np.concatenate(fpr_all))\n",
        "        mean_tpr = np.zeros_like(all_fpr)\n",
        "        for tpr_i, fpr_i in zip(tpr_all, fpr_all):\n",
        "            mean_tpr += np.interp(all_fpr, fpr_i, tpr_i)\n",
        "        mean_tpr /= Y_bin.shape[1]\n",
        "        macro_auc = _auc(Y_bin, Yp, average='macro', multi_class='ovr')  # same as earlier\n",
        "        roc_data[name] = (all_fpr, mean_tpr, macro_auc)\n",
        "\n",
        "    cm = confusion_matrix(y_true_all, y_pred_all, labels=labels_global)\n",
        "    conf_matrices[name] = cm\n",
        "\n",
        "df_final = pd.DataFrame(final_results).sort_values(\"F1_mean\", ascending=False).round(3)\n",
        "df_final\n"
      ],
      "metadata": {
        "id": "qPNa5FQXpSth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da093872"
      },
      "source": [
        "### Loading a Saved Model for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import clone\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# Create Website-Ready Model Package\n",
        "# ========================================\n",
        "def create_website_package(models, X, y, df_final,\n",
        "                           best_model_name=None,\n",
        "                           out_path=\"website_model_package.pkl\"):\n",
        "    \"\"\"\n",
        "    Create a clean, website-ready package.\n",
        "\n",
        "    - models: dict of trained models  (e.g. {\"LogReg\": logreg_clf, ...})\n",
        "    - X: pandas DataFrame with training features\n",
        "    - y: array-like target\n",
        "    - df_final: DataFrame with columns [\"Model\", \"Acc_mean\", \"F1_mean\"]\n",
        "    - best_model_name: name in `models` (if None ‚Üí take best F1 from df_final)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Choose the best model\n",
        "    if best_model_name is None:\n",
        "        # Sort df_final by F1_mean descending and take top model name\n",
        "        best_row = df_final.sort_values(\"F1_mean\", ascending=False).iloc[0]\n",
        "        best_model_name = best_row[\"Model\"]\n",
        "    else:\n",
        "        best_row = df_final[df_final[\"Model\"] == best_model_name].iloc[0]\n",
        "\n",
        "    # 2) Clone the model so we don't overwrite the original\n",
        "    base_model = models[best_model_name]\n",
        "    best_model = clone(base_model)\n",
        "\n",
        "    # 3) Build a sklearn Pipeline with scaler + model\n",
        "    website_pipeline = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", best_model),\n",
        "    ])\n",
        "\n",
        "    # 4) Fit on full dataset\n",
        "    website_pipeline.fit(X, y)\n",
        "\n",
        "    # 5) Build a clean dict to save\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    website_package = {\n",
        "        # The main object used by the website\n",
        "        \"pipeline\": website_pipeline,              # sklearn Pipeline (safe to unpickle)\n",
        "\n",
        "        # Feature information\n",
        "        \"feature_names\": feature_names,\n",
        "        \"feature_ranges\": {\n",
        "            col: {\n",
        "                \"min\": float(X[col].min()),\n",
        "                \"max\": float(X[col].max())\n",
        "            }\n",
        "            for col in feature_names\n",
        "        },\n",
        "\n",
        "        # Model metadata\n",
        "        \"model_type\": best_model_name,\n",
        "        \"is_binary\": len(np.unique(y)) == 2,\n",
        "        \"classes\": np.unique(y).tolist(),\n",
        "\n",
        "        # Performance info (for display on website)\n",
        "        \"performance\": {\n",
        "            \"accuracy\": float(best_row[\"Acc_mean\"]),\n",
        "            \"f1_score\": float(best_row[\"F1_mean\"]),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # 6) Save with joblib (recommended for sklearn objects)\n",
        "    joblib.dump(website_package, out_path, compress=3)\n",
        "    print(f\"üéØ Website package saved ‚Üí {out_path}\")\n",
        "    print(\n",
        "        f\"   Best model: {best_model_name} \"\n",
        "        f\"(F1: {website_package['performance']['f1_score']:.3f})\"\n",
        "    )\n",
        "\n",
        "    return website_package\n",
        "\n",
        "\n",
        "# ===== Example call (adapt to your notebook) =====\n",
        "best_model_name = df_final.sort_values(\"F1_mean\", ascending=False).iloc[0][\"Model\"]\n",
        "website_package = create_website_package(models, X, y, df_final, best_model_name)\n"
      ],
      "metadata": {
        "id": "8D3-ZgB4WC5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_model_loading.py\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "def test_website_model(path=\"website_model_package.pkl\"):\n",
        "    try:\n",
        "        model_data = joblib.load(path)\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "        print(f\"ü§ñ Model Type: {model_data['model_type']}\")\n",
        "        print(f\"üìä Features: {len(model_data['feature_names'])}\")\n",
        "        print(f\"üìà Performance: F1={model_data['performance']['f1_score']:.3f}\")\n",
        "        print(f\"üî¢ Binary Classification: {model_data['is_binary']}\")\n",
        "        print(f\"üéØ Classes: {model_data['classes']}\")\n",
        "\n",
        "        # Test prediction with dummy data\n",
        "        dummy = pd.DataFrame(\n",
        "            [[0.0] * len(model_data[\"feature_names\"])],\n",
        "            columns=model_data[\"feature_names\"]\n",
        "        )\n",
        "        pred = model_data[\"pipeline\"].predict(dummy)\n",
        "        print(f\"üß™ Test prediction: {pred[0]}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run locally in your training environment\n",
        "test_website_model()\n"
      ],
      "metadata": {
        "id": "mBLa3bUxY1c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "CT2epT0jsxIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "RNG = 42\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
        "\n",
        "labels_global = np.unique(y)\n",
        "cm_mean_per_model = {}   # {model_name: mean_normalized_cm}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n===== {name} (mean CM over 5 folds) =====\")\n",
        "    cms_norm = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y), start=1):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # raw CM for this fold\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=labels_global)\n",
        "\n",
        "        # row-normalize (per true class) to get recall-style percentages\n",
        "        row_sums = cm.sum(axis=1, keepdims=True).clip(min=1)\n",
        "        cm_norm = cm.astype(float) / row_sums\n",
        "        cms_norm.append(cm_norm)\n",
        "\n",
        "    # mean normalized CM across folds\n",
        "    cm_mean = np.mean(np.stack(cms_norm, axis=0), axis=0)  # shape (C, C)\n",
        "    cm_mean_per_model[name] = cm_mean\n",
        "\n"
      ],
      "metadata": {
        "id": "N_CNeJEqszn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from math import ceil\n",
        "\n",
        "num_models = len(cm_mean_per_model)\n",
        "cols = 3\n",
        "rows = ceil(num_models / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 6*rows), dpi=500)\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (name, cm_mean) in enumerate(cm_mean_per_model.items()):\n",
        "    # scale to percent for display\n",
        "    cm_percent = cm_mean * 100.0\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
        "                                  display_labels=labels_global)\n",
        "    disp.plot(ax=axes[i],\n",
        "              cmap='viridis',      # bright, matches your style\n",
        "              colorbar=False,\n",
        "              values_format=\".0f\") # show as integer percent; we'll append the % sign below\n",
        "\n",
        "    axes[i].set_title(name, fontsize=18, weight='bold')\n",
        "    axes[i].set_xlabel(\"Predicted label\", fontsize=14)\n",
        "    axes[i].set_ylabel(\"True label\", fontsize=14)\n",
        "\n",
        "    # enlarge and add % sign to each cell value\n",
        "    for txt in axes[i].texts:\n",
        "        txt.set_text(f\"{txt.get_text()}%\")\n",
        "        txt.set_fontsize(25)\n",
        "        txt.set_weight('bold')\n",
        "\n",
        "# hide unused subplots (if any)\n",
        "for j in range(i+1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "fig.suptitle(\"Confusion Matrices over 5 Folds ‚Äî All Models\",\n",
        "             fontsize=22, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.92)\n",
        "plt.show()\n",
        "\n",
        "# Optional: save as PDF (vector quality for Overleaf)\n",
        "fig.savefig(\"mean_confusion_matrices_all_models.pdf\",\n",
        "            format='pdf', dpi=500)\n"
      ],
      "metadata": {
        "id": "0ZrXbizys4qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# ROC Curves ‚Äî All Models (Publication Quality)\n",
        "# ========================================\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Define elegant color palette (colorblind-friendly)\n",
        "colors = cycle([\n",
        "    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\",\n",
        "    \"#9467bd\", \"#8c564b\", \"#e377c2\", \"#7f7f7f\",\n",
        "    \"#bcbd22\", \"#17becf\"\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(8, 7), dpi=500)\n",
        "\n",
        "# Plot each model‚Äôs ROC\n",
        "for (name, (fpr, tpr, roc_auc)), color in zip(roc_data.items(), colors):\n",
        "    plt.plot(\n",
        "        fpr, tpr, color=color, lw=2.5,\n",
        "        label=f\"{name} (AUC = {roc_auc:.3f})\"\n",
        "    )\n",
        "\n",
        "# Add diagonal reference line\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1.2, alpha=0.6)\n",
        "\n",
        "# === Style & labels ===\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) ‚Äî All Models\", fontsize=10, weight='bold', pad=15)\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "\n",
        "# === Grid and legend ===\n",
        "plt.grid(True, which=\"major\", linestyle=\"--\", linewidth=0.6, alpha=0.4)\n",
        "plt.legend(\n",
        "    loc=\"lower right\",\n",
        "    fontsize=11,\n",
        "    frameon=True,\n",
        "    edgecolor='gray',\n",
        "    fancybox=True,\n",
        "    shadow=False\n",
        ")\n",
        "\n",
        "# === Adjust margins and save ===\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"roc_curves.pdf\", format='pdf', dpi=500)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mfuOB7IPpuzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability"
      ],
      "metadata": {
        "id": "T-c0zpFjP2ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn shap lime eli5 captum"
      ],
      "metadata": {
        "id": "lymVPfynThHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_uncorr.columns"
      ],
      "metadata": {
        "id": "vKL2N__XRGih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "feature_names = ['Age(years)', 'Sex', 'HeartRate', 'Respiratoryrate',\n",
        "       'Rectaltemperature', 'Gutsounds', 'Digitalpulses', 'Bodyweight(kg)',\n",
        "       'BodyConditionScoring(outof9)', 'LengthRF', 'LengthLF', 'LengthRH',\n",
        "       'WidthRF', 'WidthLF', 'WidthRH', 'HTRF', 'HTRH', 'LERF']\n"
      ],
      "metadata": {
        "id": "-4Fr9fMXQltd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "8Ax_wV4CQ1Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=2000, class_weight='balanced', random_state=RNG)\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "9781IoaifyMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC # Import SVC\n",
        "\n",
        "# Train the model\n",
        "model = SVC(kernel=\"linear\") # Changed linear to \"linear\" and added quotes\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "eDjkTpnzkBTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train the model\n",
        "model = XGBClassifier(\n",
        "        n_estimators=400, max_depth=5, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_lambda=1.0, objective=\"binary:logistic\" if n_classes==2 else \"multi:softprob\",\n",
        "        eval_metric=\"logloss\", random_state=RNG, n_jobs=-1\n",
        "    )\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "v4M_hgtWkJAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train the model\n",
        "model = GradientBoostingClassifier(\n",
        "        n_estimators=300, learning_rate=0.05, max_depth=3, random_state=RNG\n",
        "    )\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "4UIqAKveke5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train the model\n",
        "model = DecisionTreeClassifier(\n",
        "        max_depth=None, class_weight='balanced', random_state=RNG\n",
        "    )\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "-VOxEDsJkmVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train the model\n",
        "model = KNeighborsClassifier(n_neighbors=2)\n",
        "model.fit(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "eli5.show_weights(model, feature_names=feature_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "4jhsc1V4k3L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lamintis"
      ],
      "metadata": {
        "id": "IO4TczhPHwcB"
      }
    }
  ]
}