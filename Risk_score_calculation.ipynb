{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xLLrLyTO5dH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import ttest_ind, chi2_contingency\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import ace_tools_open as tools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Laminitis_27_Oct/preprocessed.xlsx')\n",
        "# Loop only over object (string) columns\n",
        "for col in df.select_dtypes(include='number').columns:\n",
        "    num_missing = df[col].isna().sum()\n",
        "    if num_missing > 0:\n",
        "        print(f\"{col}: {num_missing} NaN values\")\n",
        "\n",
        "\n",
        "# Optional: Check the data types after conversion\n",
        "# print(df.dtypes)\n"
      ],
      "metadata": {
        "id": "Zu_D9BNTSVLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "cfzISbkPFAfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Class'].value_counts()\n"
      ],
      "metadata": {
        "id": "H-pcgF1LaP62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features = ['Age(years)', 'Sex', 'Respiratoryrate', 'Rectaltemperature',\n",
        "#     'Gutsounds', 'Digitalpulses', 'BodyConditionScoring(outof9)',\n",
        "#     'LengthLF', 'WidthLF', 'HTRF','HTRH', 'LERF','LELF','LERH', 'LELH']\n",
        "# x = df[features]\n",
        "# df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "x = df.drop(columns=['Class'])  # all features\n",
        "y = df['Class']                 # label\n",
        "# # Apply SMOTE\n",
        "# smote = SMOTE(random_state=42)\n",
        "# x, y = smote.fit_resample(x, y)\n"
      ],
      "metadata": {
        "id": "6iGrqIHqSD9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check new class distribution\n",
        "print(\"Class distribution after SMOTE:\")\n",
        "print(pd.Series(y).value_counts())\n"
      ],
      "metadata": {
        "id": "yfRmL0jS7hyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = df.iloc[:,:25]\n",
        "# y = df['Class']\n",
        "# x.shape, y.shape"
      ],
      "metadata": {
        "id": "MUTtpWhqS6oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4653aaf"
      },
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = x.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "display(correlation_matrix)\n",
        "\n",
        "# Optional: Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Features in x')\n",
        "plt.savefig('correlation_matrix.png',dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "4SgrFhfg30WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Age(years)', 'Sex', 'Respiratoryrate', 'Rectaltemperature',\n",
        "            'Gutsounds', 'Digitalpulses', 'Bodyweight(kg)',\n",
        "            'BodyConditionScoring(outof9)',\n",
        "            'LengthRH',  'LengthLH', 'WidthLF','WidthRF', 'HTRF', 'HTLH',\n",
        "            'LERF', 'LELF', 'LERH', 'LELH']"
      ],
      "metadata": {
        "id": "spAgXVMay3lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and fit the Logistic Regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# # Identify columns with non-numeric values\n",
        "# for col in x.columns:\n",
        "#     try:\n",
        "#         x[col] = pd.to_numeric(x[col])\n",
        "#     except ValueError:\n",
        "#         print(f\"Column '{col}' contains non-numeric values.\")\n",
        "\n",
        "# Convert non-numeric values to NaN and then fill with 0\n",
        "# x = x.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "x = x[features]\n",
        "logreg.fit(x, y)\n",
        "\n",
        "# Access the coefficients and intercept\n",
        "coefficients = logreg.coef_[0]\n",
        "intercept = logreg.intercept_[0]\n",
        "\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Intercept:\", intercept)\n",
        "\n",
        "# Optional: Associate coefficients with feature names\n",
        "feature_names = x.columns\n",
        "coef_df = pd.DataFrame(zip(feature_names, coefficients), columns=['features', 'Coefficient'])\n",
        "print(\"\\nCoefficients with Feature Names:\")\n",
        "print(coef_df)"
      ],
      "metadata": {
        "id": "zp-XY9EWSdLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_beta_dict(feature_names, coefficients, round_digits=6):\n",
        "    \"\"\"\n",
        "    Converts feature names and coefficients into a dictionary.\n",
        "\n",
        "    Args:\n",
        "        feature_names (list): List of feature/column names.\n",
        "        coefficients (list or array): Corresponding list/array of coefficients.\n",
        "        round_digits (int): Number of decimal digits to round the coefficients.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping feature names to rounded coefficient values.\n",
        "    \"\"\"\n",
        "    return dict(zip(feature_names, [round(float(coef), round_digits) for coef in coefficients]))\n",
        "\n",
        "beta_values = convert_to_beta_dict(feature_names, coefficients)\n",
        "\n",
        "print(beta_values)"
      ],
      "metadata": {
        "id": "kNBIptFtuDcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_column_ranges(df):\n",
        "    \"\"\"\n",
        "    Finds the range (min and max) for all columns in a DataFrame, excluding 'lameness_risk_binary'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the min and max values for each column.\n",
        "    \"\"\"\n",
        "    # Select columns excluding 'lameness_risk_binary'\n",
        "    numeric_columns = df.columns\n",
        "\n",
        "    # Calculate the min and max for each column\n",
        "    min_values = df[numeric_columns].min()\n",
        "    max_values = df[numeric_columns].max()\n",
        "\n",
        "    # Create a DataFrame to store the results\n",
        "    range_df = pd.DataFrame({'Min': min_values, 'Max': max_values})\n",
        "\n",
        "    return range_df\n",
        "\n",
        "# Assuming your DataFrame is named 'df'\n",
        "column_ranges = find_column_ranges(x)\n",
        "print(column_ranges)"
      ],
      "metadata": {
        "id": "e619-sSBZRju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Age(years)', 'Sex', 'Respiratoryrate', 'Rectaltemperature',\n",
        "            'Gutsounds', 'Digitalpulses', 'Bodyweight(kg)',\n",
        "            'BodyConditionScoring(outof9)',\n",
        "            'LengthRH',  'LengthLH', 'WidthLF','WidthRF', 'HTRF', 'HTLH',\n",
        "            'LERF', 'LELF', 'LERH', 'LELH']"
      ],
      "metadata": {
        "id": "NqqeptiS1Gv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define reference categories and Wj values for all features\n",
        "reference_data_all = [\n",
        "    # Age(years)\n",
        "    [\"Age(years)\", \"0-4\", 2],\n",
        "    [\"Age(years)\", \"5-9\", 7],\n",
        "    [\"Age(years)\", \"10-14\", 12],\n",
        "    [\"Age(years)\", \"15-20\", 17],\n",
        "    [\"Age(years)\", \"21-25\", 23],\n",
        "\n",
        "    # Sex (categorical: 0–3)\n",
        "    *[[\"Sex\", str(sex), sex] for sex in range(0, 4)],\n",
        "\n",
        "    # Respiratory Rate\n",
        "    [\"Respiratoryrate\", \"8-16\", 12],\n",
        "    [\"Respiratoryrate\", \"17-30\", 24],\n",
        "    [\"Respiratoryrate\", \"31-50\", 40],\n",
        "    [\"Respiratoryrate\", \"51-76\", 63],\n",
        "\n",
        "    # Rectal Temperature\n",
        "    [\"Rectaltemperature\", \"35.8-36.9\", 36.4],\n",
        "    [\"Rectaltemperature\", \"37.0-38.0\", 37.5],\n",
        "    [\"Rectaltemperature\", \"38.1-38.5\", 38.3],\n",
        "\n",
        "    # Gutsounds (binary)\n",
        "    [\"Gutsounds\", \"0\", 0],\n",
        "    [\"Gutsounds\", \"1\", 1],\n",
        "\n",
        "    # Digitalpulses (binary)\n",
        "    [\"Digitalpulses\", \"0\", 0],\n",
        "    [\"Digitalpulses\", \"1\", 1],\n",
        "\n",
        "    # Bodyweight(kg)\n",
        "    [\"Bodyweight(kg)\", \"245-300\", 272],\n",
        "    [\"Bodyweight(kg)\", \"301-400\", 350],\n",
        "    [\"Bodyweight(kg)\", \"401-500\", 450],\n",
        "    [\"Bodyweight(kg)\", \"501-567\", 534],\n",
        "\n",
        "    # Body Condition Scoring(outof9)\n",
        "    [\"BodyConditionScoring(outof9)\", \"1-3\", 2],\n",
        "    [\"BodyConditionScoring(outof9)\", \"4-6\", 5],\n",
        "    [\"BodyConditionScoring(outof9)\", \"7-9\", 8],\n",
        "\n",
        "    # Length features\n",
        "    *[\n",
        "        [feature, \"0-5\", 2.5] for feature in ['LengthRH',  'LengthLH']\n",
        "    ] + [\n",
        "        [feature, \"5.1-10\", 7.5] for feature in ['LengthRH',  'LengthLH']\n",
        "    ] + [\n",
        "        [feature, \"10.1-14.5\", 12.3] for feature in ['LengthRH',  'LengthLH']\n",
        "    ],\n",
        "\n",
        "    # Width features\n",
        "    *[\n",
        "        [feature, \"0-5\", 2.5] for feature in ['WidthLF','WidthRF']\n",
        "    ] + [\n",
        "        [feature, \"5.1-10\", 7.5] for feature in ['WidthLF','WidthRF']\n",
        "    ] + [\n",
        "        [feature, \"10.1-14.0\", 12.0] for feature in ['WidthLF','WidthRF']\n",
        "    ],\n",
        "\n",
        "    # Heat in feet (ordinal 0–3)\n",
        "    # Heat in feet (ordinal, based on actual data)\n",
        "\n",
        "        [\"HTRF\", \"0\", 0],\n",
        "        [\"HTRF\", \"1\", 1],\n",
        "        [\"HTRF\", \"2\", 2],\n",
        "\n",
        "        [\"HTLF\", \"0\", 0],\n",
        "        [\"HTLF\", \"1\", 1],\n",
        "        [\"HTLF\", \"2\", 2],\n",
        "        [\"HTLF\", \"3\", 3],\n",
        "\n",
        "        [\"HTRH\", \"0\", 0],\n",
        "        [\"HTRH\", \"1\", 1],\n",
        "\n",
        "        [\"HTLH\", \"0\", 0],\n",
        "        [\"HTLH\", \"1\", 1],\n",
        "\n",
        "    # Lesion scores (ordinal 0–4)\n",
        "\n",
        "        [\"LERF\", \"0\", 0],\n",
        "        [\"LERF\", \"3\", 3],\n",
        "        [\"LERF\", \"4\", 4],\n",
        "\n",
        "        [\"LELF\", \"0\", 0],\n",
        "        [\"LELF\", \"3\", 3],\n",
        "        [\"LELF\", \"4\", 4],\n",
        "\n",
        "        [\"LERH\", \"0\", 0],\n",
        "        [\"LERH\", \"3\", 3],\n",
        "        [\"LERH\", \"4\", 4],\n",
        "\n",
        "        [\"LELH\", \"0\", 0],\n",
        "        [\"LELH\", \"3\", 3],\n",
        "        [\"LELH\", \"4\", 4],\n",
        "]\n",
        "\n",
        "# Create the reference table as a DataFrame\n",
        "reference_table = pd.DataFrame(reference_data_all, columns=[\"Risk factor\", \"Categories\", \"Reference value (Wj)\"])\n",
        "\n",
        "# Example: print the first few rows\n",
        "reference_table\n"
      ],
      "metadata": {
        "id": "JNgFC1uCrlJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def calculate_beta_values(reference_table, beta_values):\n",
        "    \"\"\"\n",
        "    Calculates the beta * (value - base) for each row in the reference table\n",
        "    and adds it as a new column, also prints the beta value in the table.\n",
        "\n",
        "    Args:\n",
        "        reference_table (pd.DataFrame): DataFrame containing risk factors,\n",
        "                                         categories, and reference values.\n",
        "        beta_values (dict): Dictionary containing beta values for each risk factor.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with the new 'Beta', and 'Beta * (Value - Base)' columns,\n",
        "                      or None if the DataFrame is empty.\n",
        "    \"\"\"\n",
        "\n",
        "    if reference_table.empty:\n",
        "        print(\"DataFrame is empty. Cannot calculate beta values.\")\n",
        "        return None\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    reference_table = reference_table.copy()\n",
        "\n",
        "    # Create a dictionary to store the base values for each risk factor\n",
        "    base_values = {}\n",
        "    for risk_factor in reference_table['Risk factor'].unique():\n",
        "        base_values[risk_factor] = reference_table[reference_table['Risk factor'] == risk_factor]['Reference value (Wj)'].iloc[0]\n",
        "\n",
        "    # Define a function to calculate the beta value for a given row\n",
        "    def calculate_beta(row):\n",
        "        risk_factor = row['Risk factor']\n",
        "        reference_value = row['Reference value (Wj)']\n",
        "        beta = beta_values.get(risk_factor)  # Use .get() to handle missing betas\n",
        "\n",
        "        if beta is not None:\n",
        "            return beta, beta * (reference_value - base_values[risk_factor])\n",
        "        else:\n",
        "            return None, None  # Or NaN, or a specific error value\n",
        "\n",
        "    # Apply the function to each row to create the new columns\n",
        "    reference_table[['Beta', 'Beta * (Value - Base)']] = reference_table.apply(calculate_beta, axis=1, result_type='expand')\n",
        "\n",
        "    return reference_table\n",
        "# def calculate_beta_values(reference_table, beta_values):\n",
        "#     \"\"\"\n",
        "#     Calculates the beta * (value - base) for each row in the reference table\n",
        "#     and adds it as a new column, also prints the beta value in the table.\n",
        "\n",
        "#     Args:\n",
        "#         reference_table (pd.DataFrame): DataFrame containing risk factors,\n",
        "#                                          categories, and reference values.\n",
        "#         beta_values (dict): Dictionary containing beta values for each risk factor.\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: DataFrame with the new 'Beta', and 'Beta * (Value - Base)' columns,\n",
        "#                       or None if the DataFrame is empty.\n",
        "#     \"\"\"\n",
        "\n",
        "#     if reference_table.empty:\n",
        "#         print(\"DataFrame is empty. Cannot calculate beta values.\")\n",
        "#         return None\n",
        "\n",
        "#     # Create a copy to avoid modifying the original DataFrame\n",
        "#     reference_table = reference_table.copy()\n",
        "\n",
        "#     # Create a dictionary to store the base values for each risk factor\n",
        "#     base_values = {}\n",
        "#     for risk_factor in reference_table['Risk factor'].unique():\n",
        "#         base_values[risk_factor] = reference_table[reference_table['Risk factor'] == risk_factor]['Reference value (Wj)'].iloc[0]\n",
        "\n",
        "#     # Define a function to calculate the beta value for a given row\n",
        "#     def calculate_beta(row):\n",
        "#         risk_factor = row['Risk factor']\n",
        "#         reference_value = row['Reference value (Wj)']\n",
        "#         beta = beta_values.get(risk_factor)  # Use .get() to handle missing betas\n",
        "\n",
        "#         if beta is not None:\n",
        "#             return beta, beta * (reference_value - base_values[risk_factor])\n",
        "#         else:\n",
        "#             return None, None  # Or NaN, or a specific error value\n",
        "\n",
        "#     # Apply the function to each row to create the new columns\n",
        "#     reference_table[['Beta', 'Beta * (Value - Base)']] = reference_table.apply(calculate_beta, axis=1, result_type='expand')\n",
        "\n",
        "#     return reference_table\n",
        "\n",
        "reference_table_with_beta = calculate_beta_values(reference_table, beta_values)\n",
        "reference_table_with_beta"
      ],
      "metadata": {
        "id": "Q0XU6pEwyFtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # STEP 2: Your feature names in order\n",
        "# feature_names = features\n",
        "# # STEP 3: Reference table you already created\n",
        "# # Make sure `reference_table` has columns: ['Risk factor', 'Categories', 'Reference value (Wj)']\n",
        "# reference_table[\"β_i\"] = reference_table[\"Risk factor\"].map(dict(zip(feature_names, coefficients)))\n",
        "\n",
        "# # STEP 4: Assign W_REF for each feature (first category assumed as reference)\n",
        "# w_ref_map = reference_table.groupby(\"Risk factor\")[\"Reference value (Wj)\"].first().to_dict()\n",
        "# reference_table[\"W_REF\"] = reference_table[\"Risk factor\"].map(w_ref_map)\n",
        "\n",
        "# # STEP 5: Compute β_i * (W_ij - W_REF)\n",
        "# reference_table[\"β(Wij - W_REF)\"] = reference_table[\"β_i\"] * (reference_table[\"Reference value (Wj)\"] - reference_table[\"W_REF\"])\n",
        "\n",
        "# # STEP 6: Calculate B as the minimum non-zero absolute value of the difference\n",
        "# non_zero_diffs = reference_table[\"β(Wij - W_REF)\"]\n",
        "# non_zero_diffs = non_zero_diffs[(non_zero_diffs != 0) & (~non_zero_diffs.isna())].abs()\n",
        "# B = non_zero_diffs.min()\n",
        "\n",
        "B = 5 * reference_table_with_beta['Beta'][1]\n",
        "\n",
        "# Output B\n",
        "print(\"Calculated B (Framingham method):\", B)\n"
      ],
      "metadata": {
        "id": "-G1d_e15tyH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_points_from_precalculated(beta_times_diff, B):\n",
        "  \"\"\"Calculates the points using the precalculated Beta * (Value - Base) and rounds to the nearest integer.\"\"\"\n",
        "  if pd.isna(beta_times_diff):\n",
        "    return 0 # Or np.nan if you prefer to keep them as missing\n",
        "\n",
        "  points = beta_times_diff / B\n",
        "  return int(np.round(points))\n",
        "\n",
        "# Apply the calculate_points_from_precalculated function to each row:\n",
        "reference_table_with_beta['Points'] = reference_table_with_beta.apply(lambda row: calculate_points_from_precalculated(row['Beta * (Value - Base)'], B), axis=1)\n",
        "\n",
        "# Print the DataFrame with the calculated points:\n",
        "reference_table_with_beta"
      ],
      "metadata": {
        "id": "PDVIG8WytyAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Intercept: 0.0112847841120903\n",
        "def calculate_risk(point_total, intercept=intercept, constant=B):\n",
        "  \"\"\"\n",
        "  Calculates the risk associated with a given point total.\n",
        "\n",
        "  Args:\n",
        "    point_total: The point total.\n",
        "    intercept: The intercept value (β₀).\n",
        "    constant: The constant B.\n",
        "\n",
        "  Returns:\n",
        "    The calculated risk (p̂).\n",
        "  \"\"\"\n",
        "  # Calculate ΣβX\n",
        "  sum_beta_x = intercept + (constant * point_total)\n",
        "\n",
        "  # Calculate risk (p̂)\n",
        "  p_hat = 1 / (1 + np.exp(-sum_beta_x))\n",
        "  return p_hat\n",
        "\n",
        "# Define the range of point totals\n",
        "point_totals = range(min(reference_table_with_beta['Points']),max(reference_table_with_beta['Points'])+2)  # Generates numbers from -7 to 9\n",
        "\n",
        "# Calculate the risk for each point total and store the results\n",
        "results = []\n",
        "for point_total in point_totals:\n",
        "  risk = calculate_risk(point_total)\n",
        "  results.append({'Point Total': point_total, 'Risk': risk})\n",
        "\n",
        "# Create a Pandas DataFrame to display the results\n",
        "risk_estimate = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(risk_estimate)"
      ],
      "metadata": {
        "id": "K3S5GdZ5seaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the two DataFrames to add the 'Risk' column\n",
        "reference_table_with_risk = pd.merge(\n",
        "    reference_table_with_beta,\n",
        "    risk_estimate,\n",
        "    left_on='Points',\n",
        "    right_on='Point Total',\n",
        "    how='left'  # Use a left merge to keep all rows from reference_table_with_beta\n",
        ")\n",
        "\n",
        "# Drop the redundant 'Point Total' column\n",
        "reference_table_with_risk = reference_table_with_risk.drop(columns=['Point Total'])\n",
        "\n",
        "# Round the 'Risk' column to two decimal places\n",
        "reference_table_with_risk['Risk'] = reference_table_with_risk['Risk'].round(2)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "reference_table_with_risk"
      ],
      "metadata": {
        "id": "NNofXm7Q1SaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table"
      ],
      "metadata": {
        "id": "OPk4bCkJpfAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your features and label are:\n",
        "x = df.drop(columns=['Class'])  # all features\n",
        "X = x.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "y = df['Class']\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "SVEW-J6BUIlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def calculate_custom_risk_score(df):\n",
        "    \"\"\"\n",
        "    Calculate custom risk score based on your available features\n",
        "    This adapts the PRISQ concept to your specific features\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        score = 0\n",
        "\n",
        "        # Age scoring (adapted from PRISQ)\n",
        "        age = row['Age(years)']\n",
        "        if 10 <= age <= 15:  # Adjusted for animal ages\n",
        "            score += 8\n",
        "        elif age > 15:\n",
        "            score += 15\n",
        "\n",
        "        # Sex scoring\n",
        "        sex = row['Sex']\n",
        "        # Assuming 1 = male, 0 = female or similar encoding\n",
        "        if sex == 1:\n",
        "            score += 3\n",
        "\n",
        "        # Body weight scoring\n",
        "        weight = row['Bodyweight(kg)']\n",
        "        if weight > 500:  # Adjust threshold based on your data distribution\n",
        "            score += 4\n",
        "        elif weight > 400:\n",
        "            score += 2\n",
        "\n",
        "        # Body condition scoring\n",
        "        bcs = row['BodyConditionScoring(outof9)']\n",
        "        if bcs >= 7:  # Overweight/obese\n",
        "            score += 5\n",
        "        elif bcs >= 5:  # Moderate condition\n",
        "            score += 2\n",
        "\n",
        "        # Temperature scoring (abnormal temperatures)\n",
        "        temp = row['Rectaltemperature']\n",
        "        if temp < 37 or temp > 38.5:  # Abnormal temperature range\n",
        "            score += 3\n",
        "\n",
        "        # Respiratory rate scoring\n",
        "        resp_rate = row['Respiratoryrate']\n",
        "        if resp_rate > 20:  # Elevated respiratory rate\n",
        "            score += 2\n",
        "\n",
        "        # Digital pulses scoring (assuming abnormal = higher risk)\n",
        "        # This might indicate circulatory issues\n",
        "        digital_pulses = row['Digitalpulses']\n",
        "        if digital_pulses > 2:  # Adjust based on your scale\n",
        "            score += 3\n",
        "\n",
        "        # Add limb measurements if they indicate asymmetry or abnormalities\n",
        "        # Calculate asymmetry between left and right limbs\n",
        "        length_asymmetry = abs(row['LengthRH'] - row['LengthLH'])\n",
        "        width_asymmetry = abs(row['WidthLF'] - row['WidthRF'])\n",
        "\n",
        "        if length_asymmetry > 1.0 or width_asymmetry > 0.5:  # Adjust thresholds\n",
        "            score += 2\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Alternative: Use machine learning to create risk scores\n",
        "def calculate_ml_risk_score(X_train, X_test, y_train):\n",
        "    \"\"\"\n",
        "    Use Random Forest to create risk scores based on feature importance\n",
        "    \"\"\"\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Get predicted probabilities for positive class\n",
        "    train_proba = rf.predict_proba(X_train)[:, 1]\n",
        "    test_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Convert probabilities to scores (0-100 scale)\n",
        "    train_scores = (train_proba * 100).astype(int)\n",
        "    test_scores = (test_proba * 100).astype(int)\n",
        "\n",
        "    return train_scores, test_scores, rf"
      ],
      "metadata": {
        "id": "gZ88rKWovl3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_custom_table_3(X_train, X_test, y_train, y_test, method='custom'):\n",
        "    \"\"\"\n",
        "    Create Table 3 for your data using custom risk scoring\n",
        "    \"\"\"\n",
        "\n",
        "    if method == 'custom':\n",
        "        # Use custom scoring system\n",
        "        train_scores = calculate_custom_risk_score(X_train)\n",
        "        test_scores = calculate_custom_risk_score(X_test)\n",
        "    else:\n",
        "        # Use ML-based scoring\n",
        "        train_scores, test_scores, model = calculate_ml_risk_score(X_train, X_test, y_train)\n",
        "\n",
        "    # Define risk categories based on your score distribution\n",
        "    def categorize_risk(score):\n",
        "        # Adjust these thresholds based on your score distribution\n",
        "        if score <= np.percentile(train_scores, 33):  # Lower third\n",
        "            return 'Low Risk'\n",
        "        elif score <= np.percentile(train_scores, 66):  # Middle third\n",
        "            return 'Moderate Risk'\n",
        "        else:  # Upper third\n",
        "            return 'High Risk'\n",
        "\n",
        "    # Categorize risks\n",
        "    train_risk = [categorize_risk(score) for score in train_scores]\n",
        "    test_risk = [categorize_risk(score) for score in test_scores]\n",
        "\n",
        "    # Create results DataFrames\n",
        "    train_df = pd.DataFrame({\n",
        "        'score': train_scores,\n",
        "        'risk_category': train_risk,\n",
        "        'actual': y_train\n",
        "    })\n",
        "\n",
        "    test_df = pd.DataFrame({\n",
        "        'score': test_scores,\n",
        "        'risk_category': test_risk,\n",
        "        'actual': y_test\n",
        "    })\n",
        "\n",
        "    # Calculate Table 3 statistics\n",
        "    def calculate_category_stats(df, dataset_name):\n",
        "        results = []\n",
        "        for category in ['Low Risk', 'Moderate Risk', 'High Risk']:\n",
        "            category_data = df[df['risk_category'] == category]\n",
        "            total = len(category_data)\n",
        "            if total > 0:\n",
        "                controls = len(category_data[category_data['actual'] == 0])\n",
        "                cases = len(category_data[category_data['actual'] == 1])\n",
        "                prevalence = (cases / total) * 100\n",
        "            else:\n",
        "                controls = cases = prevalence = 0\n",
        "\n",
        "            results.append({\n",
        "                'Dataset': dataset_name,\n",
        "                'Risk Level': category,\n",
        "                'Controls': controls,\n",
        "                'Cases': cases,\n",
        "                'Total': total,\n",
        "                'Prevalence (%)': round(prevalence, 2)\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    # Generate table data\n",
        "    table_data = (calculate_category_stats(train_df, 'Training') +\n",
        "                  calculate_category_stats(test_df, 'Testing'))\n",
        "\n",
        "    table_3 = pd.DataFrame(table_data)\n",
        "\n",
        "    return table_3, train_df, test_df\n",
        "\n",
        "# Usage\n",
        "table_3, train_df, test_df = create_custom_table_3(X_train, X_test, y_train, y_test, method='custom')\n",
        "print(\"Your Custom Table 3:\")\n",
        "print(table_3)"
      ],
      "metadata": {
        "id": "FqfIDIgbw-Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_custom_table_3(table_3, title_suffix=\"Your Dataset\"):\n",
        "    \"\"\"\n",
        "    Plot your custom Table 3 data\n",
        "    \"\"\"\n",
        "    # Separate training and testing data\n",
        "    train_data = table_3[table_3['Dataset'] == 'Training']\n",
        "    test_data = table_3[table_3['Dataset'] == 'Testing']\n",
        "\n",
        "    # Create subplots\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Plot 1: Prevalence bar chart (Training)\n",
        "    categories = train_data['Risk Level'].tolist()\n",
        "    train_prevalence = train_data['Prevalence (%)'].tolist()\n",
        "\n",
        "    bars1 = ax1.bar(categories, train_prevalence,\n",
        "                   color=['green', 'orange', 'red'], alpha=0.7)\n",
        "    ax1.set_title(f'Training Dataset - Prevalence by Risk Category\\n{title_suffix}',\n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Prevalence (%)')\n",
        "    max_prevalence = max(train_prevalence) if train_prevalence else 100\n",
        "    ax1.set_ylim(0, max_prevalence * 1.2)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars1, train_prevalence):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 2: Prevalence bar chart (Testing)\n",
        "    test_prevalence = test_data['Prevalence (%)'].tolist()\n",
        "\n",
        "    bars2 = ax2.bar(categories, test_prevalence,\n",
        "                   color=['green', 'orange', 'red'], alpha=0.7)\n",
        "    ax2.set_title(f'Testing Dataset - Prevalence by Risk Category\\n{title_suffix}',\n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Prevalence (%)')\n",
        "    max_prevalence_test = max(test_prevalence) if test_prevalence else 100\n",
        "    ax2.set_ylim(0, max_prevalence_test * 1.2)\n",
        "\n",
        "    for bar, value in zip(bars2, test_prevalence):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 3: Stacked bar chart (Training)\n",
        "    train_controls = train_data['Controls'].tolist()\n",
        "    train_cases = train_data['Cases'].tolist()\n",
        "    train_totals = [c + cs for c, cs in zip(train_controls, train_cases)]\n",
        "\n",
        "    ax3.bar(categories, train_controls, label='Controls', color='lightblue', alpha=0.8)\n",
        "    ax3.bar(categories, train_cases, bottom=train_controls, label='Cases', color='coral', alpha=0.8)\n",
        "    ax3.set_title('Training Dataset - Distribution', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('Number of Subjects')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add prevalence percentages - FIXED THIS PART\n",
        "    for i, (control, case, prev, total) in enumerate(zip(train_controls, train_cases, train_prevalence, train_totals)):\n",
        "        ax3.text(i, total + max(train_totals)*0.05, f'{prev}%', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=10)\n",
        "\n",
        "    # Plot 4: Stacked bar chart (Testing)\n",
        "    test_controls = test_data['Controls'].tolist()\n",
        "    test_cases = test_data['Cases'].tolist()\n",
        "    test_totals = [c + cs for c, cs in zip(test_controls, test_cases)]\n",
        "\n",
        "    ax4.bar(categories, test_controls, label='Controls', color='lightblue', alpha=0.8)\n",
        "    ax4.bar(categories, test_cases, bottom=test_controls, label='Cases', color='coral', alpha=0.8)\n",
        "    ax4.set_title('Testing Dataset - Distribution', fontsize=12, fontweight='bold')\n",
        "    ax4.set_ylabel('Number of Subjects')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add prevalence percentages - FIXED THIS PART\n",
        "    for i, (control, case, prev, total) in enumerate(zip(test_controls, test_cases, test_prevalence, test_totals)):\n",
        "        ax4.text(i, total + max(test_totals)*0.05, f'{prev}%', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Generate the plots\n",
        "fig = plot_custom_table_3(table_3, \"Your Clinical Features\")"
      ],
      "metadata": {
        "id": "Xk0sfVHpxCWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_risk_stratification(table_3):\n",
        "    \"\"\"\n",
        "    Analyze how well your risk stratification works\n",
        "    \"\"\"\n",
        "    print(\"=== RISK STRATIFICATION ANALYSIS ===\")\n",
        "\n",
        "    for dataset in ['Training', 'Testing']:\n",
        "        data = table_3[table_3['Dataset'] == dataset]\n",
        "        low_risk_prev = data[data['Risk Level'] == 'Low Risk']['Prevalence (%)'].values[0]\n",
        "        high_risk_prev = data[data['Risk Level'] == 'High Risk']['Prevalence (%)'].values[0]\n",
        "\n",
        "        risk_ratio = high_risk_prev / low_risk_prev if low_risk_prev > 0 else float('inf')\n",
        "\n",
        "        print(f\"\\n{dataset} Dataset:\")\n",
        "        print(f\"Low Risk Prevalence: {low_risk_prev}%\")\n",
        "        print(f\"High Risk Prevalence: {high_risk_prev}%\")\n",
        "        print(f\"Risk Ratio (High/Low): {risk_ratio:.2f}\")\n",
        "\n",
        "        if risk_ratio > 2:\n",
        "            print(\"✓ Good risk stratification\")\n",
        "        elif risk_ratio > 1.5:\n",
        "            print(\"○ Moderate risk stratification\")\n",
        "        else:\n",
        "            print(\"✗ Poor risk stratification\")\n",
        "\n",
        "# Run analysis\n",
        "analyze_risk_stratification(table_3)"
      ],
      "metadata": {
        "id": "s4l3ZtpjxJ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second method"
      ],
      "metadata": {
        "id": "qkllaKX0yQbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_custom_risk_score(df):\n",
        "    \"\"\"\n",
        "    CORRECT VERSION: Calculate risk score using your actual features\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        score = 0\n",
        "\n",
        "        # 1. AGE SCORING (High importance)\n",
        "        age = row['Age(years)']\n",
        "        if age >= 15:    # Older animals higher risk\n",
        "            score += 20\n",
        "        elif age >= 10:\n",
        "            score += 12\n",
        "        elif age >= 5:\n",
        "            score += 6\n",
        "\n",
        "        # 2. BODY CONDITION SCORING (High importance)\n",
        "        bcs = row['BodyConditionScoring(outof9)']\n",
        "        if bcs >= 7:     # Overweight/obese\n",
        "            score += 8\n",
        "        elif bcs >= 5:   # Moderate\n",
        "            score += 4\n",
        "        # Low BCS (<5) gets 0 points\n",
        "\n",
        "        # 3. SEX SCORING\n",
        "        sex = row['Sex']\n",
        "        if sex == 1:     # Male (adjust if your encoding is different)\n",
        "            score += 3\n",
        "\n",
        "        # 4. VITAL SIGNS SCORING\n",
        "        # Heart rate (adjust thresholds based on normal ranges for your species)\n",
        "        hr = row['HeartRate']\n",
        "        if hr > 50 or hr < 30:  # Abnormal heart rate\n",
        "            score += 3\n",
        "\n",
        "        # Respiratory rate\n",
        "        resp = row['Respiratoryrate']\n",
        "        if resp > 25 or resp < 10:  # Abnormal respiratory rate\n",
        "            score += 3\n",
        "\n",
        "        # Temperature\n",
        "        temp = row['Rectaltemperature']\n",
        "        if temp < 37.0 or temp > 38.5:  # Abnormal temperature\n",
        "            score += 3\n",
        "\n",
        "        # 5. DIGITAL PULSES (circulatory assessment)\n",
        "        pulses = row['Digitalpulses']\n",
        "        if pulses > 2:  # Abnormal pulses (adjust threshold)\n",
        "            score += 4\n",
        "\n",
        "        # 6. BODY WEIGHT\n",
        "        weight = row['Bodyweight(kg)']\n",
        "        if weight > 450:  # Heavy weight (adjust threshold)\n",
        "            score += 3\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def create_final_risk_categories(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    FINAL CORRECT VERSION: Create risk categories with validation\n",
        "    \"\"\"\n",
        "    # Calculate scores\n",
        "    train_scores = calculate_custom_risk_score(X_train)\n",
        "    test_scores = calculate_custom_risk_score(X_test)\n",
        "\n",
        "    print(\"=== SCORE DISTRIBUTION ===\")\n",
        "    print(f\"Training - Min: {min(train_scores)}, Max: {max(train_scores)}, Mean: {np.mean(train_scores):.2f}\")\n",
        "    print(f\"Testing  - Min: {min(test_scores)}, Max: {max(test_scores)}, Mean: {np.mean(test_scores):.2f}\")\n",
        "\n",
        "    # Use percentile-based categorization (MOST RELIABLE APPROACH)\n",
        "    p33 = np.percentile(train_scores, 33)\n",
        "    p66 = np.percentile(train_scores, 66)\n",
        "\n",
        "    print(f\"\\n=== AUTOMATIC RISK THRESHOLDS ===\")\n",
        "    print(f\"Low Risk: score ≤ {p33:.1f}\")\n",
        "    print(f\"Moderate Risk: {p33:.1f} < score ≤ {p66:.1f}\")\n",
        "    print(f\"High Risk: score > {p66:.1f}\")\n",
        "\n",
        "    def categorize_risk(scores):\n",
        "        categories = []\n",
        "        for score in scores:\n",
        "            if score <= p33:\n",
        "                categories.append('Low Risk')\n",
        "            elif score <= p66:\n",
        "                categories.append('Moderate Risk')\n",
        "            else:\n",
        "                categories.append('High Risk')\n",
        "        return categories\n",
        "\n",
        "    # Apply categorization\n",
        "    train_risk = categorize_risk(train_scores)\n",
        "    test_risk = categorize_risk(test_scores)\n",
        "\n",
        "    # Create results DataFrames\n",
        "    train_df = pd.DataFrame({\n",
        "        'score': train_scores,\n",
        "        'risk_category': train_risk,\n",
        "        'actual': y_train.values\n",
        "    })\n",
        "\n",
        "    test_df = pd.DataFrame({\n",
        "        'score': test_scores,\n",
        "        'risk_category': test_risk,\n",
        "        'actual': y_test.values\n",
        "    })\n",
        "\n",
        "    return train_df, test_df, train_scores, test_scores\n",
        "\n",
        "# RUN THE CORRECT IMPLEMENTATION\n",
        "print(\"CREATING RISK CATEGORIES...\")\n",
        "train_df, test_df, train_scores, test_scores = create_final_risk_categories(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"\\n✅ RISK CATEGORIES CREATED SUCCESSFULLY!\")\n",
        "\n",
        "def create_verified_table_3(train_df, test_df):\n",
        "    \"\"\"\n",
        "    CREATE VERIFIED TABLE 3 for your data\n",
        "    \"\"\"\n",
        "    def calculate_stats(df, dataset_name):\n",
        "        results = []\n",
        "        for category in ['Low Risk', 'Moderate Risk', 'High Risk']:\n",
        "            cat_data = df[df['risk_category'] == category]\n",
        "            total = len(cat_data)\n",
        "            controls = len(cat_data[cat_data['actual'] == 0])\n",
        "            cases = len(cat_data[cat_data['actual'] == 1])\n",
        "            prevalence = (cases / total * 100) if total > 0 else 0\n",
        "\n",
        "            results.append({\n",
        "                'Dataset': dataset_name,\n",
        "                'Risk Level': category,\n",
        "                'Controls': controls,\n",
        "                'Cases': cases,\n",
        "                'Total': total,\n",
        "                'Prevalence (%)': round(prevalence, 2)\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    # Generate table data\n",
        "    table_data = calculate_stats(train_df, 'Training') + calculate_stats(test_df, 'Testing')\n",
        "    table_3 = pd.DataFrame(table_data)\n",
        "\n",
        "    return table_3\n",
        "\n",
        "# Create your Table 3\n",
        "table_3 = create_verified_table_3(train_df, test_df)\n",
        "print(\"\\nYOUR TABLE 3:\")\n",
        "print(table_3)\n",
        "\n",
        "\n",
        "def plot_final_results(table_3):\n",
        "    \"\"\"\n",
        "    ERROR-FREE plotting function\n",
        "    \"\"\"\n",
        "    # Separate data\n",
        "    train_data = table_3[table_3['Dataset'] == 'Training']\n",
        "    test_data = table_3[table_3['Dataset'] == 'Testing']\n",
        "\n",
        "    categories = train_data['Risk Level'].tolist()\n",
        "    train_prev = train_data['Prevalence (%)'].tolist()\n",
        "    test_prev = test_data['Prevalence (%)'].tolist()\n",
        "\n",
        "    # Create plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Training data\n",
        "    bars1 = ax1.bar(categories, train_prev, color=['green', 'orange', 'red'], alpha=0.7)\n",
        "    ax1.set_title('Training Dataset\\nPrediabetes Prevalence by Risk Category', fontweight='bold')\n",
        "    ax1.set_ylabel('Prevalence (%)')\n",
        "    ax1.set_ylim(0, max(train_prev + test_prev) * 1.2)\n",
        "\n",
        "    for bar, value in zip(bars1, train_prev):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Testing data\n",
        "    bars2 = ax2.bar(categories, test_prev, color=['green', 'orange', 'red'], alpha=0.7)\n",
        "    ax2.set_title('Testing Dataset\\nPrediabetes Prevalence by Risk Category', fontweight='bold')\n",
        "    ax2.set_ylabel('Prevalence (%)')\n",
        "    ax2.set_ylim(0, max(train_prev + test_prev) * 1.2)\n",
        "\n",
        "    for bar, value in zip(bars2, test_prev):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Plot your results\n",
        "print(\"\\nGENERATING PLOTS...\")\n",
        "fig = plot_final_results(table_3)\n",
        "\n",
        "def validate_results(train_df, test_df, table_3):\n",
        "    \"\"\"\n",
        "    VALIDATE that everything is correct\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VALIDATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check category distributions\n",
        "    print(\"\\nTRAINING DATA DISTRIBUTION:\")\n",
        "    train_counts = train_df['risk_category'].value_counts()\n",
        "    for category in ['Low Risk', 'Moderate Risk', 'High Risk']:\n",
        "        count = train_counts.get(category, 0)\n",
        "        print(f\"  {category}: {count} subjects ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\nTESTING DATA DISTRIBUTION:\")\n",
        "    test_counts = test_df['risk_category'].value_counts()\n",
        "    for category in ['Low Risk', 'Moderate Risk', 'High Risk']:\n",
        "        count = test_counts.get(category, 0)\n",
        "        print(f\"  {category}: {count} subjects ({count/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "    # Check risk stratification\n",
        "    print(\"\\nRISK STRATIFICATION PERFORMANCE:\")\n",
        "    for dataset in ['Training', 'Testing']:\n",
        "        data = table_3[table_3['Dataset'] == dataset]\n",
        "        low_risk_prev = data[data['Risk Level'] == 'Low Risk']['Prevalence (%)'].values[0]\n",
        "        high_risk_prev = data[data['Risk Level'] == 'High Risk']['Prevalence (%)'].values[0]\n",
        "\n",
        "        risk_ratio = high_risk_prev / low_risk_prev if low_risk_prev > 0 else float('inf')\n",
        "\n",
        "        print(f\"  {dataset}: Low Risk = {low_risk_prev}%, High Risk = {high_risk_prev}%\")\n",
        "        print(f\"  Risk Ratio (High/Low): {risk_ratio:.2f}x\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Run validation\n",
        "validate_results(train_df, test_df, table_3)"
      ],
      "metadata": {
        "id": "Cb7ssylkx-w7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}